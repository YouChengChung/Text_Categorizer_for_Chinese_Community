{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "from config import mysql_config  as dbconfig\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from shutil import ignore_patterns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.tokens = {}\n",
    "        self.read_tokens()\n",
    "\n",
    "    def read_tokens(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    if '=' in line:\n",
    "                        key, value = line.strip().split('=')\n",
    "                        self.tokens[key.strip()] = value.strip().strip(\"'\").strip('\"')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{self.file_path}' not found.\")\n",
    "            self.tokens = {}\n",
    "\n",
    "    def get_token_value(self, token_name):\n",
    "        return self.tokens.get(token_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File 'huggingface_token.txt' not found.\n",
      "Token: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d145a0634d5d41b5aa565cdd1de77413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "file_path = 'huggingface_token.txt'\n",
    "reader = TokenReader(file_path)\n",
    "\n",
    "HF_token_value = reader.get_token_value('HF_token')\n",
    "print(\"Token:\", HF_token_value)\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.df.iloc[index]['title']\n",
    "        \n",
    "        # print(\"Dataset O label = {}\".format(label))\n",
    "        token = self.tokenizer(text, padding=True, truncation=True, max_length=32)\n",
    "        # print(token)\n",
    "        input_ids = token['input_ids']\n",
    "        token_type_ids = token['token_type_ids']\n",
    "        \n",
    "\n",
    "        attention_mask = token['attention_mask']\n",
    "\n",
    "        label = self.df.iloc[index]['label']\n",
    "        label = torch.LongTensor([label])\n",
    "\n",
    "        # print(\"Dataset label = {}\".format(label))\n",
    "\n",
    "        return (input_ids,token_type_ids, attention_mask, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(datas):\n",
    "    # print(datas)\n",
    "    input_ids = [torch.Tensor(i[0]) for i in datas]\n",
    "    token_ids = [torch.Tensor(i[1]) for i in datas]\n",
    "    attention_mask = [torch.Tensor(i[2]) for i in datas]\n",
    "\n",
    "    if datas[0][3] is not None:\n",
    "        labels = torch.stack([i[3] for i in datas])\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    input_ids_tensors = pad_sequence(input_ids, batch_first=True)\n",
    "    token_ids_tensors = pad_sequence(token_ids, batch_first=True)\n",
    "    attention_mask_tensors = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "    input_ids_tensors = input_ids_tensors.to(torch.long)\n",
    "    token_ids_tensors = token_ids_tensors.to(torch.long)\n",
    "    attention_mask_tensors = attention_mask_tensors.to(torch.long)\n",
    "\n",
    "    res = {\n",
    "        \"input_ids\": input_ids_tensors,\n",
    "        \"token_type_ids\": token_ids_tensors,\n",
    "        \"attention_mask\": attention_mask_tensors,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user = dbconfig['user']\n",
    "password = dbconfig['password']\n",
    "host = dbconfig['host']\n",
    "port = dbconfig['port']\n",
    "database = dbconfig['database']\n",
    "database_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "class DBConnecter:\n",
    "    def __init__(self, database_url):\n",
    "        \"\"\"\n",
    "        mysql+pymysql://username:password@host:port/database\n",
    "        \"\"\"\n",
    "        self.database_url = database_url\n",
    "        try:\n",
    "            self.engine = create_engine(database_url)\n",
    "            print(\"Connect to database successfully\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fail to create database connecter engine to DB：{e}\")\n",
    "\n",
    "    def connection_info(self):\n",
    "        \"\"\"\n",
    "        測試數據庫連接。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                version = conn.execute(text(\"SELECT VERSION();\"))\n",
    "                print(f\"Connect to DB successfully, DB version：{version.fetchone()[0]}\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fail to connect to DB：{e}\")\n",
    "\n",
    "    def get_insert_row_query(self, table, columns):\n",
    "        column_str = \", \".join(columns)\n",
    "        placeholder_str = \", \".join([f\":{col}\" for col in columns])\n",
    "        add_new_row_query = f\"INSERT INTO {table}({column_str}) VALUES({placeholder_str})\"\n",
    "        return add_new_row_query\n",
    "\n",
    "    def run_no_return_query(self, query, values):\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                # 確保 values 是一個字典\n",
    "                query = text(query)\n",
    "                conn.execute(query, values)\n",
    "                conn.commit() \n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"操作失敗：{e}\")\n",
    "\n",
    "    def run_query(self,sql):\n",
    "\n",
    "        sql = text(sql)\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                query_result = pd.read_sql(sql, conn)\n",
    "                return query_result # 返回查詢結果的列表\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"數據獲取失敗：{e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(df):\n",
    "    # '\\n'\n",
    "    for i in range(len(df)):\n",
    "        df['title'][i] = df['title'][i].replace('\\n','')\n",
    "\n",
    "    # 公告\n",
    "    for i in range(len(df)):\n",
    "        if (\"[公告]\" in df['title'][i]):\n",
    "            # print(df['title'][i])\n",
    "            df = df.drop(i)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_encoder(df):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['board'].unique())\n",
    "\n",
    "    label_class = list(label_encoder.classes_)\n",
    "    label_id = label_encoder.transform(label_class)\n",
    "\n",
    "    label2id_ = {label_: int(id_) for label_, id_ in zip(label_class, label_id)}\n",
    "    id2label_ = {int(id_): label_ for label_, id_ in zip(label_class, label_id)}\n",
    "\n",
    "    return label2id_,id2label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(pretain_model,df,label2id_,id2label_):\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretain_model)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretain_model,\n",
    "            num_labels = len(df['label'].unique()),\n",
    "            label2id = label2id_,\n",
    "            id2label = id2label_,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    print(model.config.num_labels)\n",
    "\n",
    "    return tokenizer,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_process(tokenizer,df):\n",
    "    \n",
    "\n",
    "    samll_df = df.sample(n=1024, random_state=42)\n",
    "    train_df, eval_df = train_test_split(samll_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_set = PTTDataset(tokenizer, train_df)\n",
    "    eval_set = PTTDataset(tokenizer,eval_df)\n",
    "\n",
    "    return train_set, eval_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def training_and_save_model_processing(model,train_arg_set,train_set,eval_set,tokenizer):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available and supports CUDA.\")\n",
    "        model = model.to(\"cuda\")\n",
    "        print('CUDA is available and can be used by',torch.cuda.device_count(),'device')\n",
    "        print('Current_device number:',torch.cuda.current_device()) #should be zero\n",
    "        print(torch.cuda.device(0))\n",
    "        print(\"PyTorch choose the GPU (what current_device number's meaning):\",torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"GPU is not available or does not support CUDA.\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_arg_set['output_dir'],\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=train_arg_set['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=train_arg_set['per_device_eval_batch_size'],\n",
    "        num_train_epochs=train_arg_set['num_train_epochs'],\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=eval_set,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=create_batch,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save locally\n",
    "    # trainer.model.save_pretrained('model/10epoch_PTT_classifier_bert-base-mengzi_model')\n",
    "    # Push To HF\n",
    "    trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to database successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>board</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[花邊] 到1/21為止所有球員TPA排行圖\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[公告] 板規10.1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[情報] SEASON Schedule January 22–23\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[公告] 板主徵選開始\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[情報] NBA Standings (Jan. 22, 2023)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] 大魯閣用icashPay滿$500贈10%op\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] === 元月全台捐血贈品 === (1/26更新)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[公告] 板規(113.1.15修訂)暨違規公告區\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[公告] 贈送集中文\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[公告] 未滿1元集點抽獎資訊集中文(置底)\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3001 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            board                                   title\n",
       "0             NBA              \\n[花邊] 到1/21為止所有球員TPA排行圖\\n\n",
       "1             NBA                         \\n[公告] 板規10.1\\n\n",
       "2             NBA  \\n[情報] SEASON Schedule January 22–23\\n\n",
       "3             NBA                         \\n[公告] 板主徵選開始\\n\n",
       "4             NBA  \\n[情報] NBA Standings (Jan. 22, 2023)\\n\n",
       "...           ...                                     ...\n",
       "2996  Lifeismoney        \\n[情報] 大魯閣用icashPay滿$500贈10%op\\n\n",
       "2997  Lifeismoney      \\n[情報] === 元月全台捐血贈品 === (1/26更新)\\n\n",
       "2998  Lifeismoney           \\n[公告] 板規(113.1.15修訂)暨違規公告區\\n\n",
       "2999  Lifeismoney                          \\n[公告] 贈送集中文\\n\n",
       "3000  Lifeismoney              \\n[公告] 未滿1元集點抽獎資訊集中文(置底)\\n\n",
       "\n",
       "[3001 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數據獲取失敗：This result object does not return rows. It has been closed automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Langboat/mengzi-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "GPU is available and supports CUDA.\n",
      "CUDA is available and can be used by 1 device\n",
      "Current_device number: 0\n",
      "<torch.cuda.device object at 0x000001E62480D450>\n",
      "PyTorch choose the GPU (what current_device number's meaning): NVIDIA GeForce RTX 2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ZProject\\NLP\\Text_Categorizer_for_Chinese_Community\\Train\\runs/10epoch_PTT_classifier_bert-base-mengzi_model is already a clone of https://huggingface.co/youchengChung/10epoch_PTT_classifier_bert-base-mengzi_model. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "c:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ca674ee0bb44629c7605687c3f677c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 37\u001b[0m\n\u001b[0;32m     30\u001b[0m train_set, eval_set \u001b[38;5;241m=\u001b[39m train_test_split_process(tokenizer,df)\n\u001b[0;32m     31\u001b[0m train_arg_set\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/10epoch_PTT_classifier_bert-base-mengzi_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_train_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_eval_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     36\u001b[0m }\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtraining_and_save_model_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_arg_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 42\u001b[0m, in \u001b[0;36mtraining_and_save_model_processing\u001b[1;34m(model, train_arg_set, train_set, eval_set, tokenizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     18\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mtrain_arg_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     19\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     33\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     34\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Save locally\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# trainer.model.save_pretrained('model/10epoch_PTT_classifier_bert-base-mengzi_model')\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Push To HF\u001b[39;00m\n\u001b[0;32m     47\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub()\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\transformers\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1784\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1788\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\accelerate\\data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 393\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\accelerate\\utils\\operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 160\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\accelerate\\utils\\operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m    160\u001b[0m         {\n\u001b[1;32m--> 161\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\accelerate\\utils\\operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    db_connector = DBConnecter(database_url)\n",
    "    data = db_connector.run_query(f\"\"\"SELECT board,title FROM nlp.PTT\"\"\")\n",
    "    display(data)\n",
    "\n",
    "    df = data_transform(data)\n",
    "\n",
    "    sql_create_ptt_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS PTT_dealed (\n",
    "                                    board text NOT NULL,\n",
    "                                    title text\n",
    "                                );\"\"\"\n",
    "    db_connector.run_query(sql_create_ptt_table_query)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        insert_dict = {\n",
    "        \"board\": str(row['board']),\n",
    "        \"title\": str(row['title'])\n",
    "                            }\n",
    "        columns = ['board', 'title']\n",
    "        add_new_row_query = db_connector.get_insert_row_query('PTT_dealed', columns)\n",
    "        db_connector.run_no_return_query(add_new_row_query, insert_dict)\n",
    "\n",
    "    label2id_,id2label_ = label_encoder(df)\n",
    "    df['label'] = df['board'].apply(lambda x: label2id_[x])\n",
    "\n",
    "    pretain_model = \"Langboat/mengzi-bert-base\"\n",
    "    tokenizer,model = model_config(pretain_model,df,label2id_,id2label_)\n",
    "\n",
    "    train_set, eval_set = train_test_split_process(tokenizer,df)\n",
    "    train_arg_set={\n",
    "        'output_dir':\"runs/10epoch_PTT_classifier_bert-base-mengzi_model\",\n",
    "        'num_train_epochs':10,\n",
    "        'per_device_train_batch_size':8,\n",
    "        'per_device_eval_batch_size':8,\n",
    "    }\n",
    "    training_and_save_model_processing(model,train_arg_set,train_set,eval_set,tokenizer)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_categorizer_for_chinese_community_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
