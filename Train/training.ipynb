{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "from config import mysql_config  as dbconfig\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from shutil import ignore_patterns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.tokens = {}\n",
    "        self.read_tokens()\n",
    "\n",
    "    def read_tokens(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    if '=' in line:\n",
    "                        key, value = line.strip().split('=')\n",
    "                        self.tokens[key.strip()] = value.strip().strip(\"'\").strip('\"')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{self.file_path}' not found.\")\n",
    "            self.tokens = {}\n",
    "\n",
    "    def get_token_value(self, token_name):\n",
    "        return self.tokens.get(token_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File 'huggingface_token.txt' not found.\n",
      "Token: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049813a9cb5c494fb8cc9600f4867e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "file_path = 'huggingface_token.txt'\n",
    "reader = TokenReader(file_path)\n",
    "\n",
    "HF_token_value = reader.get_token_value('HF_token')\n",
    "print(\"Token:\", HF_token_value)\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.df.iloc[index]['title']\n",
    "        \n",
    "        # print(\"Dataset O label = {}\".format(label))\n",
    "        token = self.tokenizer(text, padding=True, truncation=True, max_length=32)\n",
    "        # print(token)\n",
    "        input_ids = token['input_ids']\n",
    "        token_type_ids = token['token_type_ids']\n",
    "        \n",
    "\n",
    "        attention_mask = token['attention_mask']\n",
    "\n",
    "        label = self.df.iloc[index]['label']\n",
    "        label = torch.LongTensor([label])\n",
    "\n",
    "        # print(\"Dataset label = {}\".format(label))\n",
    "\n",
    "        return (input_ids,token_type_ids, attention_mask, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(datas):\n",
    "    # print(datas)\n",
    "    input_ids = [torch.Tensor(i[0]) for i in datas]\n",
    "    token_ids = [torch.Tensor(i[1]) for i in datas]\n",
    "    attention_mask = [torch.Tensor(i[2]) for i in datas]\n",
    "\n",
    "    if datas[0][3] is not None:\n",
    "        labels = torch.stack([i[3] for i in datas])\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    input_ids_tensors = pad_sequence(input_ids, batch_first=True)\n",
    "    token_ids_tensors = pad_sequence(token_ids, batch_first=True)\n",
    "    attention_mask_tensors = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "    input_ids_tensors = input_ids_tensors.to(torch.long)\n",
    "    token_ids_tensors = token_ids_tensors.to(torch.long)\n",
    "    attention_mask_tensors = attention_mask_tensors.to(torch.long)\n",
    "\n",
    "    res = {\n",
    "        \"input_ids\": input_ids_tensors,\n",
    "        \"token_type_ids\": token_ids_tensors,\n",
    "        \"attention_mask\": attention_mask_tensors,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user = dbconfig['user']\n",
    "password = dbconfig['password']\n",
    "host = dbconfig['host']\n",
    "port = dbconfig['port']\n",
    "database = dbconfig['database']\n",
    "database_url = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "class DBConnecter:\n",
    "    def __init__(self, database_url):\n",
    "        \"\"\"\n",
    "        mysql+pymysql://username:password@host:port/database\n",
    "        \"\"\"\n",
    "        self.database_url = database_url\n",
    "        try:\n",
    "            self.engine = create_engine(database_url)\n",
    "            print(\"Connect to database successfully\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fail to create database connecter engine to DB：{e}\")\n",
    "\n",
    "    def connection_info(self):\n",
    "        \"\"\"\n",
    "        測試數據庫連接。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                version = conn.execute(text(\"SELECT VERSION();\"))\n",
    "                print(f\"Connect to DB successfully, DB version：{version.fetchone()[0]}\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fail to connect to DB：{e}\")\n",
    "\n",
    "    def get_insert_row_query(self, table, columns):\n",
    "        column_str = \", \".join(columns)\n",
    "        placeholder_str = \", \".join([f\":{col}\" for col in columns])\n",
    "        add_new_row_query = f\"INSERT INTO {table}({column_str}) VALUES({placeholder_str})\"\n",
    "        return add_new_row_query\n",
    "\n",
    "    def run_no_return_query(self, query, values=None):\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                query = text(query)\n",
    "                if values:\n",
    "                    conn.execute(query, values)\n",
    "                else:\n",
    "                    conn.execute(text(query))\n",
    "                conn.execute()\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"操作失敗：{e}\")\n",
    "\n",
    "    def run_query(self,sql):\n",
    "\n",
    "        sql = text(sql)\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                query_result = pd.read_sql(sql, conn)\n",
    "                return query_result # 返回查詢結果的列表\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"數據獲取失敗：{e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(df):\n",
    "    # '\\n'\n",
    "    for i in range(len(df)):\n",
    "        df['title'][i] = df['title'][i].replace('\\n','')\n",
    "\n",
    "    # 公告\n",
    "    for i in range(len(df)):\n",
    "        if (\"[公告]\" in df['title'][i]):\n",
    "            # print(df['title'][i])\n",
    "            df = df.drop(i)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_encoder(df):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['board'].unique())\n",
    "\n",
    "    label_class = list(label_encoder.classes_)\n",
    "    label_id = label_encoder.transform(label_class)\n",
    "\n",
    "    label2id_ = {label_: int(id_) for label_, id_ in zip(label_class, label_id)}\n",
    "    id2label_ = {int(id_): label_ for label_, id_ in zip(label_class, label_id)}\n",
    "\n",
    "    return label2id_,id2label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(pretain_model,df,label2id_,id2label_):\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretain_model)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretain_model,\n",
    "            num_labels = len(df['label'].unique()),\n",
    "            label2id = label2id_,\n",
    "            id2label = id2label_,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    print(model.config.num_labels)\n",
    "\n",
    "    return tokenizer,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_process(tokenizer,df):\n",
    "    \n",
    "\n",
    "    samll_df = df.sample(n=1024, random_state=42)\n",
    "    train_df, eval_df = train_test_split(samll_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_set = PTTDataset(tokenizer, train_df)\n",
    "    eval_set = PTTDataset(tokenizer,eval_df)\n",
    "\n",
    "    return train_set, eval_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def training_and_save_model_processing(model,train_arg_set,train_set,eval_set,tokenizer):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available and supports CUDA.\")\n",
    "        model = model.to(\"cuda\")\n",
    "        print('CUDA is available and can be used by',torch.cuda.device_count(),'device')\n",
    "        print('Current_device number:',torch.cuda.current_device()) #should be zero\n",
    "        print(torch.cuda.device(0))\n",
    "        print(\"PyTorch choose the GPU (what current_device number's meaning):\",torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"GPU is not available or does not support CUDA.\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_arg_set['output_dir'],\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=train_arg_set['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=train_arg_set['per_device_eval_batch_size'],\n",
    "        num_train_epochs=train_arg_set['num_train_epochs'],\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=eval_set,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=create_batch,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save locally\n",
    "    # trainer.model.save_pretrained('model/10epoch_PTT_classifier_bert-base-mengzi_model')\n",
    "    # Push To HF\n",
    "    trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to database successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>board</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[花邊] 到1/21為止所有球員TPA排行圖\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[公告] 板規10.1\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[情報] SEASON Schedule January 22–23\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[公告] 板主徵選開始\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBA</td>\n",
       "      <td>\\n[情報] NBA Standings (Jan. 22, 2023)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] 蝦皮12/25優惠分享\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] PChome24力尾牙周 搶券+限時優惠\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] Friday影音免費兩個月序號(已滿)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] 全家優惠趣又有買一送一可以領\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>Lifeismoney</td>\n",
       "      <td>\\n[情報] 下載/登入 HOLA和樂家居APP 贈50購物金 \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5833 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            board                                   title\n",
       "0             NBA              \\n[花邊] 到1/21為止所有球員TPA排行圖\\n\n",
       "1             NBA                         \\n[公告] 板規10.1\\n\n",
       "2             NBA  \\n[情報] SEASON Schedule January 22–23\\n\n",
       "3             NBA                         \\n[公告] 板主徵選開始\\n\n",
       "4             NBA  \\n[情報] NBA Standings (Jan. 22, 2023)\\n\n",
       "...           ...                                     ...\n",
       "5828  Lifeismoney                    \\n[情報] 蝦皮12/25優惠分享\\n\n",
       "5829  Lifeismoney           \\n[情報] PChome24力尾牙周 搶券+限時優惠\\n\n",
       "5830  Lifeismoney            \\n[情報] Friday影音免費兩個月序號(已滿)\\n\n",
       "5831  Lifeismoney                 \\n[情報] 全家優惠趣又有買一送一可以領\\n\n",
       "5832  Lifeismoney      \\n[情報] 下載/登入 HOLA和樂家居APP 贈50購物金 \\n\n",
       "\n",
       "[5833 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數據獲取失敗：This result object does not return rows. It has been closed automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Langboat/mengzi-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "GPU is available and supports CUDA.\n",
      "CUDA is available and can be used by 1 device\n",
      "Current_device number: 0\n",
      "<torch.cuda.device object at 0x00000236B3B57110>\n",
      "PyTorch choose the GPU (what current_device number's meaning): NVIDIA GeForce RTX 2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ZProject\\NLP\\Text_Categorizer_for_Chinese_Community\\Train\\runs/10epoch_PTT_classifier_bert-base-mengzi_model is already a clone of https://huggingface.co/youchengChung/10epoch_PTT_classifier_bert-base-mengzi_model. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "c:\\Users\\yoche\\.conda\\envs\\text_categorizer_for_chinese_community_env\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7070c2467e48beb2cffe1b760c97ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c5820f91034676a642e475f2de08c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4336712956428528, 'eval_accuracy': 0.828125, 'eval_runtime': 27.2712, 'eval_samples_per_second': 18.774, 'eval_steps_per_second': 2.347, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264124a216ef4f9c98c4671d2bc122dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30797746777534485, 'eval_accuracy': 0.9140625, 'eval_runtime': 26.7209, 'eval_samples_per_second': 19.161, 'eval_steps_per_second': 2.395, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2d609ee2b04ae588a78b53df12c3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28179290890693665, 'eval_accuracy': 0.935546875, 'eval_runtime': 25.9808, 'eval_samples_per_second': 19.707, 'eval_steps_per_second': 2.463, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a2ee5357a4125895a6f7745c8a3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3227519690990448, 'eval_accuracy': 0.9375, 'eval_runtime': 26.3011, 'eval_samples_per_second': 19.467, 'eval_steps_per_second': 2.433, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020e151594534580be6ed903e44c805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3591858446598053, 'eval_accuracy': 0.9296875, 'eval_runtime': 26.8122, 'eval_samples_per_second': 19.096, 'eval_steps_per_second': 2.387, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc109460c92411b886aa044ca71fd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26493895053863525, 'eval_accuracy': 0.947265625, 'eval_runtime': 26.6838, 'eval_samples_per_second': 19.188, 'eval_steps_per_second': 2.398, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e078caf2de78455db3fe4353bbc0a7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31566813588142395, 'eval_accuracy': 0.9453125, 'eval_runtime': 26.8113, 'eval_samples_per_second': 19.096, 'eval_steps_per_second': 2.387, 'epoch': 7.0}\n",
      "{'loss': 0.1921, 'learning_rate': 1.09375e-05, 'epoch': 7.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d892e97d51a41e899377f107ac7f941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3208877444267273, 'eval_accuracy': 0.9453125, 'eval_runtime': 26.9934, 'eval_samples_per_second': 18.968, 'eval_steps_per_second': 2.371, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b7412fdbfe4b3daa607fff8fc9636a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3199528455734253, 'eval_accuracy': 0.9453125, 'eval_runtime': 26.7142, 'eval_samples_per_second': 19.166, 'eval_steps_per_second': 2.396, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ca6b47d2194a4190f0275e302da402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32007890939712524, 'eval_accuracy': 0.9453125, 'eval_runtime': 26.9232, 'eval_samples_per_second': 19.017, 'eval_steps_per_second': 2.377, 'epoch': 10.0}\n",
      "{'train_runtime': 2314.792, 'train_samples_per_second': 2.212, 'train_steps_per_second': 0.276, 'train_loss': 0.15026879290817305, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59ddf1595564aa9b927a0d1ac015d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/youchengChung/10epoch_PTT_classifier_bert-base-mengzi_model\n",
      "   a4e704e..41273bb  main -> main\n",
      "\n",
      "To https://huggingface.co/youchengChung/10epoch_PTT_classifier_bert-base-mengzi_model\n",
      "   41273bb..c80e1db  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    db_connector = DBConnecter(database_url)\n",
    "    data = db_connector.run_query(f\"\"\"SELECT board,title FROM nlp.PTT\"\"\")\n",
    "    display(data)\n",
    "\n",
    "    df = data_transform(data)\n",
    "\n",
    "    sql_create_ptt_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS PTT_dealed (\n",
    "                                    board text NOT NULL,\n",
    "                                    title text\n",
    "                                );\"\"\"\n",
    "    db_connector.run_query(sql_create_ptt_table_query)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        insert_dict = {\n",
    "        \"board\": str(row['board']),\n",
    "        \"title\": str(row['title'])\n",
    "                            }\n",
    "        columns = ['board', 'title']\n",
    "        add_new_row_query = db_connector.get_insert_row_query('PTT_dealed', columns)\n",
    "        db_connector.run_no_return_query(add_new_row_query, insert_dict)\n",
    "\n",
    "    label2id_,id2label_ = label_encoder(df)\n",
    "    df['label'] = df['board'].apply(lambda x: label2id_[x])\n",
    "\n",
    "    pretain_model = \"Langboat/mengzi-bert-base\"\n",
    "    tokenizer,model = model_config(pretain_model,df,label2id_,id2label_)\n",
    "\n",
    "    train_set, eval_set = train_test_split_process(tokenizer,df)\n",
    "    train_arg_set={\n",
    "        'output_dir':\"runs/10epoch_PTT_classifier_bert-base-mengzi_model\",\n",
    "        'num_train_epochs':10,\n",
    "        'per_device_train_batch_size':8,\n",
    "        'per_device_eval_batch_size':8,\n",
    "    }\n",
    "    training_and_save_model_processing(model,train_arg_set,train_set,eval_set,tokenizer)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_categorizer_for_chinese_community_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
