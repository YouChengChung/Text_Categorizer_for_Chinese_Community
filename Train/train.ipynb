{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.tokens = {}\n",
    "        self.read_tokens()\n",
    "\n",
    "    def read_tokens(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    if '=' in line:\n",
    "                        key, value = line.strip().split('=')\n",
    "                        self.tokens[key.strip()] = value.strip().strip(\"'\").strip('\"')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{self.file_path}' not found.\")\n",
    "            self.tokens = {}\n",
    "\n",
    "    def get_token_value(self, token_name):\n",
    "        return self.tokens.get(token_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseHandler:\n",
    "    def __init__(self, db_name):\n",
    "        self.db_file = db_name\n",
    "        self.conn = None\n",
    "\n",
    "    def create_connection(self):\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(self.db_file)\n",
    "        except:\n",
    "            print(\"Connection error\")\n",
    "\n",
    "    def create_table(self, create_table_query):\n",
    "        try:\n",
    "            c = self.conn.cursor()\n",
    "            c.execute(create_table_query)\n",
    "            # self.conn.commit()  # Uncomment this line if you want to commit changes immediately\n",
    "        except Exception as ex:\n",
    "            print(f\"Create table error:{ex}\")\n",
    "\n",
    "    def add_new_row(self, table,insert_list):\n",
    "        # Insert new data\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            str_insert_list = \"'\" + \"','\".join(insert_list) + \"'\"\n",
    "            add_new_row_query = f\"\"\"INSERT INTO {table}(board,title)\n",
    "                                   VALUES({str_insert_list})\"\"\"\n",
    "\n",
    "            cursor.execute(add_new_row_query)\n",
    "            self.conn.commit()\n",
    "        except :\n",
    "            print('?')\n",
    "            print(insert_list)\n",
    "    \n",
    "    def get_data(self,query):\n",
    "        query_result = pd.read_sql(query,self.conn)\n",
    "        return query_result\n",
    "\n",
    "    def close_connection(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.df.iloc[index]['title']\n",
    "        \n",
    "        # print(\"Dataset O label = {}\".format(label))\n",
    "        token = self.tokenizer(text, padding=True, truncation=True, max_length=32)\n",
    "        # print(token)\n",
    "        input_ids = token['input_ids']\n",
    "        token_type_ids = token['token_type_ids']\n",
    "        \n",
    "\n",
    "        attention_mask = token['attention_mask']\n",
    "\n",
    "        label = self.df.iloc[index]['label']\n",
    "        label = torch.LongTensor([label])\n",
    "\n",
    "        # print(\"Dataset label = {}\".format(label))\n",
    "\n",
    "        return (input_ids,token_type_ids, attention_mask, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(datas):\n",
    "    # print(datas)\n",
    "    input_ids = [torch.Tensor(i[0]) for i in datas]\n",
    "    token_ids = [torch.Tensor(i[1]) for i in datas]\n",
    "    attention_mask = [torch.Tensor(i[2]) for i in datas]\n",
    "\n",
    "    if datas[0][3] is not None:\n",
    "        labels = torch.stack([i[3] for i in datas])\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    input_ids_tensors = pad_sequence(input_ids, batch_first=True)\n",
    "    token_ids_tensors = pad_sequence(token_ids, batch_first=True)\n",
    "    attention_mask_tensors = pad_sequence(attention_mask, batch_first=True)\n",
    "\n",
    "    input_ids_tensors = input_ids_tensors.to(torch.long)\n",
    "    token_ids_tensors = token_ids_tensors.to(torch.long)\n",
    "    attention_mask_tensors = attention_mask_tensors.to(torch.long)\n",
    "\n",
    "    res = {\n",
    "        \"input_ids\": input_ids_tensors,\n",
    "        \"token_type_ids\": token_ids_tensors,\n",
    "        \"attention_mask\": attention_mask_tensors,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login HF_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "file_path = 'huggingface_token.txt'\n",
    "reader = TokenReader(file_path)\n",
    "\n",
    "HF_token_value = reader.get_token_value('HF_token')\n",
    "print(\"Token:\", HF_token_value)\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"Crawler\\mydatabase1.db\"\n",
    "current_folder = os.getcwd()  # 取得目前程式碼所在的資料夾路徑\n",
    "database_name = os.path.join(current_folder, '..',database_path)  # 組合路徑\n",
    "print(database_name)\n",
    "\n",
    "# 確認目標database是否存在\n",
    "if os.path.exists(database_name):\n",
    "    # 在這裡進行你的讀取資料夾操作\n",
    "    print(\"database存在，可以進行讀取資料夾的操作。\")\n",
    "else:\n",
    "    print(\"database不存在。\")\n",
    "\n",
    "\n",
    "\n",
    "handler = DatabaseHandler(database_name)\n",
    "# create a database connection\n",
    "handler.create_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT board,title FROM PTT\"\"\"\n",
    "df = handler.get_data(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of post on each board\n",
    "board_count={}\n",
    "for i in df['board'].unique():\n",
    "    board_count[i] = list(df['board']).count(i)\n",
    "print(board_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. '\\n' in title\n",
    "2. drop 公告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '\\n'\n",
    "for i in range(len(df)):\n",
    "    df['title'][i] = df['title'][i].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公告\n",
    "for i in range(len(df)):\n",
    "    if (\"[公告]\" in df['title'][i]):\n",
    "        # print(df['title'][i])\n",
    "        df = df.drop(i)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dealed data\n",
    "\n",
    "sql_create_ptt_table_query = f\"\"\"CREATE TABLE IF NOT EXISTS PTT_dealed (\n",
    "                                    board text NOT NULL,\n",
    "                                    title text\n",
    "                                );\"\"\"\n",
    "\n",
    "# create tables\n",
    "handler.create_table(sql_create_ptt_table_query)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    insert_list = [str(row['board']), str(row['title'])]\n",
    "    handler.add_new_row('PTT_dealed',insert_list)\n",
    "\n",
    "handler.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['board'].unique())\n",
    "\n",
    "label_class = list(label_encoder.classes_)\n",
    "label_id = label_encoder.transform(label_class)\n",
    "\n",
    "label2id_ = {label_: int(id_) for label_, id_ in zip(label_class, label_id)}\n",
    "id2label_ = {int(id_): label_ for label_, id_ in zip(label_class, label_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['board'].apply(lambda x: label2id_[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from shutil import ignore_patterns\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/mengzi-bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"Langboat/mengzi-bert-base\",\n",
    "        num_labels = len(df['label'].unique()),\n",
    "        label2id = label2id_,\n",
    "        id2label = id2label_,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "print(model.config.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取模型的配置\n",
    "config = model.config\n",
    "\n",
    "# 查詢模型架構\n",
    "print(config)\n",
    "\n",
    "# 查詢模型的層數\n",
    "num_layers = model.config.num_hidden_layers\n",
    "print(\"模型的層數：\", num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "samll_df = df.sample(n=1024, random_state=42)\n",
    "train_df, eval_df = train_test_split(samll_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_set = PTTDataset(tokenizer, train_df)\n",
    "eval_set = PTTDataset(tokenizer,eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available and supports CUDA.\")\n",
    "    model = model.to(\"cuda\")\n",
    "    print('CUDA is available and can be used by',torch.cuda.device_count(),'device')\n",
    "    print('Current_device number:',torch.cuda.current_device()) #should be zero\n",
    "    print(torch.cuda.device(0))\n",
    "    print(\"PyTorch choose the GPU (what current_device number's meaning):\",torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is not available or does not support CUDA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print('iteration:',len(train_set)/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"runs/10epoch_PTT_classifier_bert-base-mengzi_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=eval_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=create_batch,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "trainer.model.save_pretrained('model/10epoch_PTT_classifier_bert-base-mengzi_model')\n",
    "# Push To HF\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postmeasurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" Re: [情報] 全家APP刮刮樂又來了\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bd7db9283e9e8085258194ee67b6ecd92b654bfc214ec6409926b342b7633d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
